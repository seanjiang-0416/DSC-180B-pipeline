{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sanity only\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce01c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def scrape_site(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract header\n",
    "        header = soup.find(['h1']).get_text().strip()\n",
    "\n",
    "        # Extract content\n",
    "        content_tags = soup.find_all(['p'])\n",
    "        content = [tag.get_text().strip().replace('\\xa0', ' ') for tag in content_tags]\n",
    "\n",
    "        # Find the keyword 'By' to extract the author's name\n",
    "        page_text = soup.get_text()\n",
    "        match = re.search(r'\\bBy\\s+([A-Za-z\\s.,]+)', page_text)\n",
    "        authors = match.group(1).strip().replace('and', ',') if match else 'Author not found'\n",
    "        author_lst = [auth.strip() for auth in authors.split(',')]\n",
    "        return header, content, author_lst\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None, None, None\n",
    "\n",
    "url = \"https://www.cnn.com/2024/01/17/politics/biden-ukraine-white-house-meeting/index.html\"\n",
    "header, content, authors = scrape_site(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893cdd7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import context_veracity\n",
    "content_concat = \" \".join(content)\n",
    "sent_score = context_veracity.sentiment_shift(content_concat)\n",
    "topic_score = context_veracity.topic_shift(content_concat)\n",
    "ner_score = context_veracity.ner_shift(content_concat)\n",
    "context_veracity_score = context_veracity.calculate_contextual_drift(sent_score, topic_score, ner_score)\n",
    "print(f\"The context veracity score of the article is {context_veracity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import political_bias\n",
    "import numpy as np\n",
    "pred_labels = []\n",
    "for paragraph in content:\n",
    "    processed_article = political_bias.preprocess_article(header, paragraph)\n",
    "    label = political_bias.predict_label(processed_article)\n",
    "    pred_labels.append(label[0])\n",
    "print(\"The political bias score of the article is \", np.mean(pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2861538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment import SentimentModel\n",
    "import pickle\n",
    "with open('models/sentiM.pkl', 'rb') as f:\n",
    "    sentM = pickle.load(f)\n",
    "pred_labels = []\n",
    "for paragraph in content:\n",
    "    label = sentM.predict_article(header, paragraph)[0]\n",
    "    pred_labels.append(label)\n",
    "print(\"The sentiment score of the article is \", np.mean(pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8e8d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "from credibility import search_wikipedia, text_embedding, preprocess_text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "with open('models/credibility_model.pkl', 'rb') as f:\n",
    "    cred_model = pickle.load(f)\n",
    "\n",
    "search_results = []\n",
    "for author in authors:\n",
    "    search_results.append(search_wikipedia(author, num_results=15))\n",
    "\n",
    "search_pd = pd.DataFrame(search_results, columns=['text'])\n",
    "embedded_result = text_embedding(search_pd['text'])[:, :50]\n",
    "cred_scores = cred_model.predict(embedded_result)\n",
    "if len(cred_scores) == 1:\n",
    "    print(f'The score of {authors[0]} is {cred_scores[0]}')\n",
    "else:\n",
    "    for i in range(len(authors)):\n",
    "        print(f'The score of {authors[i]} is {cred_scores[i]}')\n",
    "    print(f'The average score is {np.mean(cred_scores)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
