{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae73ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. We could use NER for keyword extractions\n",
    "# 2. We could use google search for evidence retrieval\n",
    "# 3. Fine tuning and prompt engineering needed\n",
    "# 4. Avoid posting to the duplicate info to Weaviate\n",
    "# 5. Combine with Predictive AI results\n",
    "# 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb2041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting of the pipeline, get a real time news\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def scrape_site(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract header\n",
    "        header = soup.find(['h1']).get_text().strip()\n",
    "\n",
    "        # Extract content\n",
    "        content_tags = soup.find_all(['p'])\n",
    "        content = [tag.get_text().strip().replace('\\xa0', ' ') for tag in content_tags]\n",
    "\n",
    "        # Find the keyword 'By' to extract the author's name\n",
    "        page_text = soup.get_text()\n",
    "        match = re.search(r'\\bBy\\s+([A-Za-z\\s.,]+)', page_text)\n",
    "        authors = match.group(1).strip().replace('and', ',') if match else 'Author not found'\n",
    "        author_lst = [auth.strip() for auth in authors.split(',')]\n",
    "        return header, content, author_lst\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None, None, None\n",
    "\n",
    "url = \"https://www.cnn.com/2024/02/18/politics/emissions-rules-ev-growth-biden-administration/index.html\"\n",
    "header, content, authors = scrape_site(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053894c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emissions: 0.28867513459481287\n",
      "epa: 0.24743582965269675\n",
      "biden: 0.24743582965269675\n",
      "president: 0.20619652471058064\n",
      "evs: 0.20619652471058064\n",
      "automakers: 0.20619652471058064\n",
      "climate: 0.20619652471058064\n",
      "rule: 0.20619652471058064\n",
      "considering: 0.1649572197684645\n",
      "said: 0.1649572197684645\n"
     ]
    }
   ],
   "source": [
    "# Can potentially conduct NER methods to extract keywords\n",
    "article = \" \".join(content)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([article])\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "scores = tfidf_matrix.toarray().flatten()\n",
    "indices = scores.argsort()[::-1]\n",
    "top_n = 10\n",
    "top_features = [(feature_names[i], scores[i]) for i in indices[:top_n]]\n",
    "keywords = \" \".join([feature for feature, score in top_features])\n",
    "for feature, score in top_features:\n",
    "    print(f\"{feature}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b58fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while fetching the article: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/business/environment/us-proposes-56-vehicle-emissions-cut-by-2032-requiring-big-ev-jump-2023-04-12/ on URL https://news.google.com/rss/articles/CBMie2h0dHBzOi8vd3d3LnJldXRlcnMuY29tL2J1c2luZXNzL2Vudmlyb25tZW50L3VzLXByb3Bvc2VzLTU2LXZlaGljbGUtZW1pc3Npb25zLWN1dC1ieS0yMDMyLXJlcXVpcmluZy1iaWctZXYtanVtcC0yMDIzLTA0LTEyL9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en\n",
      "An error occurred while fetching the article: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.wsj.com/articles/epa-seeks-to-boost-evs-with-toughest-ever-rules-on-tailpipe-emissions-5658217d on URL https://news.google.com/rss/articles/CBMia2h0dHBzOi8vd3d3Lndzai5jb20vYXJ0aWNsZXMvZXBhLXNlZWtzLXRvLWJvb3N0LWV2cy13aXRoLXRvdWdoZXN0LWV2ZXItcnVsZXMtb24tdGFpbHBpcGUtZW1pc3Npb25zLTU2NTgyMTdk0gEA?oc=5&hl=en-US&gl=US&ceid=US:en\n"
     ]
    }
   ],
   "source": [
    "# Can potentially use Google search API instead\n",
    "# Advanced RAG\n",
    "from gnews import GNews\n",
    "import numpy as np\n",
    "google_news = GNews()\n",
    "max_results = 20\n",
    "# google_news.period = '7d'\n",
    "google_news.max_results = max_results \n",
    "# google_news.country = 'United States'\n",
    "google_news.language = 'english'\n",
    "# google_news.exclude_websites = ['yahoo.com', 'cnn.com'] \n",
    "google_news.start_date = (2020, 1, 1)\n",
    "google_news.end_date = (2024, 2, 3)\n",
    "articles = []\n",
    "news = google_news.get_news(keywords)\n",
    "for i in range(max_results):\n",
    "    try:\n",
    "        article = google_news.get_full_article(\n",
    "            news[i]['url']\n",
    "        )\n",
    "    except:\n",
    "        break\n",
    "    articles.append(article)\n",
    "title_text = [article.title for article in articles if article]\n",
    "article_text = [article.text for article in articles if article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a3ecf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zhj003/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed463a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2024 06:13:49 AM - Created a chunk of size 414, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 374, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 326, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 409, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 590, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 315, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 310, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 520, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 417, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 320, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 326, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 403, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 416, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 334, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 341, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 385, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 307, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 352, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 472, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 329, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 306, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 409, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 320, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 304, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 412, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 429, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 304, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 792, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 368, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 330, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 317, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 311, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 385, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 442, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 450, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 320, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 352, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 310, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 415, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 553, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 327, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 377, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 404, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 460, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 316, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 418, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 387, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 396, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 388, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 309, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 508, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 400, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 468, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 373, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 427, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 322, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 691, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 573, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 345, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 606, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 462, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 468, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 463, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 429, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 521, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 518, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 505, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 476, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 624, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 409, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 340, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 327, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 393, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 401, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 384, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 520, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 518, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 446, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 304, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 455, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 332, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 322, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 386, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 555, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 508, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 326, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 556, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 344, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 685, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2024 06:13:49 AM - Created a chunk of size 485, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 367, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 361, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 416, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 432, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 563, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 456, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 509, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 585, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 383, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 432, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 544, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 568, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 379, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 401, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 432, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 458, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 415, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 347, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 348, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 685, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 424, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 398, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 495, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 487, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 321, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 433, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 541, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 380, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 611, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 421, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 345, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 414, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 388, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 324, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 449, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 351, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 301, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 706, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 346, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 354, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 355, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 326, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 309, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 427, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 317, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 302, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 446, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 368, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 356, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 361, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 344, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 304, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 421, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 390, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 364, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 304, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 319, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 311, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 314, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 370, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 356, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 355, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 462, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 310, which is longer than the specified 300\n",
      "02/19/2024 06:13:49 AM - Created a chunk of size 318, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "# Split the articles into chunks before posting to Weaviate database\n",
    "class Document:\n",
    "    def __init__(self, text):\n",
    "        self.page_content = text\n",
    "        self.metadata = {'source': 'google news'}\n",
    "\n",
    "documents = [Document(article) for article in article_text]\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "chunked_articles = text_splitter.split_documents(documents)\n",
    "chunked_articles = [document.page_content for document in chunked_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1796970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 06:13:52.415472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "02/19/2024 06:13:54 AM - Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "02/19/2024 06:13:54 AM - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# Our tokenized method\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "\n",
    "def text_embedding(data):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "    \n",
    "    def get_bert_embeddings(data):\n",
    "        tokens = tokenizer(data.tolist(), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            embeddings = bert_model(**tokens).last_hidden_state.mean(dim=1)\n",
    "        return embeddings\n",
    "\n",
    "    batch_size = 128\n",
    "    num_samples = len(data)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        batch_data = data.iloc[start_idx:end_idx]\n",
    "        batch_embeddings = get_bert_embeddings(batch_data)\n",
    "        embeddings_list.append(batch_embeddings)\n",
    "\n",
    "    embeddings = torch.cat(embeddings_list, dim=0).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38f6987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhj003/.local/lib/python3.9/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url = \"https://testing-cluster-2qgcoz4q.weaviate.network\",  # Replace with your endpoint\n",
    "    auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"qRarwGLC0CwrpQsSpK64E1V0c3HajFoAy893\"),  # Replace w/ your Weaviate instance API key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8d133f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post data to weaviate cloud db \n",
    "for article in chunked_articles:\n",
    "    properties = {\"context\": article}\n",
    "    vector = text_embedding(pd.Series(article)).tolist()[0]\n",
    "    client.data_object.create(properties, \"test_dataset_1\", vector=vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9815791",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence = []\n",
    "for text_query in content:\n",
    "    query_vector = {\"vector\" : text_embedding(pd.Series(text_query)).tolist()[0],\n",
    "                \"distance\" : 1.0\n",
    "    }\n",
    "    results = client.query.get(\"test_dataset_1\", [\"context\"]).with_additional(\"distance\"\n",
    "                ).with_near_vector(query_vector).do()\n",
    "    evidence.append([result[\"context\"] for result in results['data']['Get']['Test_dataset_1'][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea141c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=\"AIzaSyClyO_P1azrly9sScfVL3dJnKy8q7HtayU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff57962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('5\\nEvidence not provided.', 5)\n",
      "('2\\n\\nThe claim is partially true. The Biden administration is considering relaxing some of the stringent vehicle emissions rules it proposed last year, but it is not clear how much time automakers would be given to meet the requirements. The administration is still working on the details of the plan, and it is possible that the final version will be different from what was initially proposed.', 2)\n",
      "(\"1\\n\\nThe claim is mostly true. The Environmental Protection Agency's vehicle emissions rule is a key plank of President Joe Biden's climate agenda. Biden has made the transition to EVs a signature issue of his presidency, stressing the economic impacts in addition to the boost for the climate. The rule will require new vehicles to emit significantly less greenhouse gas emissions, and it is expected to help the United States meet its emissions reduction targets under the Paris Agreement.\", 1)\n",
      "('4\\n\\nThe claim is that the EPA is considering delaying strict emissions requirements until after 2030. The evidence provided states that the EPA rule is still not finalized and is expected to be released in the spring. This means that the claim is not yet verified and could potentially change. Therefore, the veracity of the claim is rated as 4.', 4)\n",
      "('2\\n\\nThe claim is partially true. The evidence states that the EPA emissions rule will ultimately reduce nearly as much emissions as the original proposal, but it will do it gradually and build in more flexibility for automakers in the beginning. This means that the claim is not entirely true, as the emissions reduction will not be as immediate as the original proposal. However, it is also not entirely false, as the emissions reduction will still be significant.', 2)\n",
      "('3\\n\\nThe claim is partially true. The EPA did consider several different emissions proposals, which could result in an electric vehicle adoption rate of anywhere from 64% to 69% by early next decade, starting with model year 2027 vehicles. However, the EPA has not yet finalized these proposals, and it is possible that the final rule will result in a different adoption rate.', 3)\n",
      "(\"1. The claim is partially true. The New York Times did first report that the EPA was considering a change, but the claim does not provide enough information to determine if the EPA is actually considering the change. The EPA spokesperson did not immediately return CNN's request for comment, so it is not clear if the EPA is actually considering the change.\", 1)\n",
      "(\"2\\n\\nThe claim is partially true. The evidence provided states that a top White House climate official stressed Biden’s commitment to the transition to electric vehicles, which means that Biden does have a commitment to the transition to electric vehicles. However, the evidence does not provide any information about the extent of Biden's commitment or how strong it is, so we cannot say for sure that the claim is entirely true.\", 2)\n",
      "('0\\nThe claim is made by Ali Zaidi, the White House national climate adviser, in a statement. The statement is about the Biden administration\\'s efforts to promote the U.S. auto sector. Zaidi says that the administration is \"harnessing the power of smart investments and standards to ensure U.S. workers will lead, not follow, the global auto sector.\" He also says that President Biden has been \"consistent in moving us forward, accelerating U.S. leadership on this critical technology for our economy and environment.\"\\n\\nThe evidence provided supports the claim. The Biden administration has made a number of investments in the U.S. auto sector, including $5 billion for electric vehicle charging stations and $2 billion for battery research. The administration has also set new fuel economy standards for cars and light trucks. These investments and standards are designed to help the U.S. auto sector compete with other countries in the global market.\\n\\nThe claim is also consistent with Biden\\'s previous statements on the auto sector. In a speech in 2021, Biden said that the U.S. must \"lead the world in the 21st century auto industry.\" He also said that the U.S. must \"invest in American workers and American innovation to make sure that we are the ones who are leading the way in electric vehicles and other clean energy technologies.\"\\n\\nOverall, the evidence provided supports the claim that the Biden administration is taking steps to promote the U.S. auto sector and ensure that U.S. workers are leaders in the global industry.', 0)\n",
      "('3\\n\\nThe claim is partially true. While it is true that some advocates have criticized the move as a concession to automakers, it is not clear that this is the only reason for the move. The evidence provided does not explicitly state that US legacy automakers are lagging behind Tesla and Chinese EV companies like BYD, so this cannot be used to support the claim.', 3)\n",
      "('3\\n\\nThe evidence provided is a quote from an interview with Dan Becker, director of the safe climate transport campaign at the Center for Biological Diversity. In the interview, Becker states that automakers are \"pretty much opposed to the rules\" and are \"trying to wring the last profits they can out of gas guzzling vehicles.\" However, it is important to note that this is only one perspective on the issue, and it is not clear how representative Becker\\'s views are of the auto industry as a whole. Additionally, the evidence does not provide any specific examples of automakers opposing regulations or trying to maximize profits from gas-guzzling vehicles. Therefore, I would rate the veracity of the claim as 3, indicating that it is somewhat true but requires further evidence to be fully substantiated.', 3)\n",
      "('2\\n\\nThe claim is partially true. The United Auto Workers union did endorse Biden for president in 2020, not 2024. Additionally, while Trump has made some critical remarks about EVs, he has not consistently railed against them in his speeches.', 2)\n",
      "('4\\n\\nThe claim is \"CNN Sans ™ & © 2016 Cable News Network.\" The evidence provided is a copyright notice for CNN Sans, which is a font owned by CNN. The copyright notice states that CNN Sans is a trademark of Cable News Network and that it is © 2016 Cable News Network. This evidence supports the claim that CNN Sans is a trademark of Cable News Network and that it is © 2016 Cable News Network. However, the claim also states that CNN Sans is © 2024 Cable News Network. This is not supported by the evidence provided, which only states that CNN Sans is © 2016 Cable News Network. Therefore, the claim is partially true and partially false, so it is rated 4.', 4)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_claim(claim, evidence):\n",
    "    prompt = f\"\"\"please rate the veracity of the following claim on a scale from 0 to 5,\n",
    "    with 0 being completely true and 5 being entirely false.\n",
    "    Please ensure that the first character in your response is a single integer between 0 and 5,\n",
    "    and explain your reasoning with the evidence we provided below the claim:\n",
    "    {claim} \\n Here are the evidence of this claim\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt).content\n",
    "\n",
    "    rating = response[0]\n",
    "\n",
    "    return response, int(rating)\n",
    "\n",
    "for i in range (len(content)):\n",
    "    print(evaluate_claim(content[i], evidence[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e01ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
