{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Claims from Liar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, test, validation data\n",
    "train_data = pd.read_csv(\"train2.tsv\",sep='\\t', header=None)\n",
    "test_data = pd.read_csv(\"test2.tsv\",sep='\\t', header=None)\n",
    "val_data = pd.read_csv(\"val2.tsv\",sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename all the columns\n",
    "train_data.rename({1: 'id', 2: 'label', 3: 'claims', 4: 'subject', 5: 'speaker', 6: 'job-title',\n",
    "           7: 'state_info', 8: 'party_affiliation', 9: 'barely_true_counts', 10: 'false_counts',\n",
    "           11: 'half_true_counts', 12: 'mostly_true_counts', 13: 'pants_on_fire_counts', 14: 'context',\n",
    "           15: 'evidence'\n",
    "          }, axis = 1, inplace = True)\n",
    "test_data.rename({1: 'id', 2: 'label', 3: 'claims', 4: 'subject', 5: 'speaker', 6: 'job-title',\n",
    "           7: 'state_info', 8: 'party_affiliation', 9: 'barely_true_counts', 10: 'false_counts',\n",
    "           11: 'half_true_counts', 12: 'mostly_true_counts', 13: 'pants_on_fire_counts', 14: 'context',\n",
    "           15: 'evidence'\n",
    "          }, axis = 1, inplace = True)\n",
    "val_data.rename({1: 'id', 2: 'label', 3: 'claims', 4: 'subject', 5: 'speaker', 6: 'job-title',\n",
    "           7: 'state_info', 8: 'party_affiliation', 9: 'barely_true_counts', 10: 'false_counts',\n",
    "           11: 'half_true_counts', 12: 'mostly_true_counts', 13: 'pants_on_fire_counts', 14: 'context',\n",
    "           15: 'evidence'\n",
    "          }, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'claims' column\n",
    "claims = train_data['claims']\n",
    "\n",
    "# Save the 'claims' column to a text file\n",
    "claims.to_csv('liar_claims.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLUSA Data Process with unreliable sources filtered out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Replace 'your_csv_file.csv' with the path to your CSV file\n",
    "csv_file_path = './polusa_balanced/2019_1.csv'\n",
    "\n",
    "# Replace 'text' with the name of the column containing the document text\n",
    "text_column_name = 'body'\n",
    "\n",
    "# The output JSONL file\n",
    "jsonl_file_path = 'polusa2019.jsonl'\n",
    "\n",
    "# List of unreliable source outlets to exclude\n",
    "unreliable_sources = ['Breitbart']\n",
    "\n",
    "with open(csv_file_path, mode='r', encoding='utf-8') as csv_file, \\\n",
    "     open(jsonl_file_path, mode='w', encoding='utf-8') as jsonl_file:\n",
    "    \n",
    "    reader = csv.DictReader(csv_file)\n",
    "    counter = 0\n",
    "    for row in reader:\n",
    "        if row['outlet'] in unreliable_sources:\n",
    "            # Skip the row if the outlet is in the list of unreliable sources\n",
    "            continue\n",
    "\n",
    "        if counter < 20000:\n",
    "            # Extract the text content from the specified column\n",
    "            text = row.get(text_column_name, '')\n",
    "\n",
    "            # Create a JSON object and write it to the JSONL file\n",
    "            json_obj = {'text': text}\n",
    "            jsonl_file.write(json.dumps(json_obj) + '\\n')\n",
    "\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(f\"Conversion complete. JSONL file saved as '{jsonl_file_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
