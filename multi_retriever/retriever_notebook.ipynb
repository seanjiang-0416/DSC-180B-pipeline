{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ab5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "from scipy import spatial\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from retriever.dense_retriever import DenseRetriever\n",
    "from retriever.sparse_retriever_fast import SparseRetrieverFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed247839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yic055/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ff14a",
   "metadata": {},
   "source": [
    "## MUSER Code Structure\n",
    "#### ref: https://github.com/Complex-data/MUSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5917ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsRetriever:\n",
    "    def __init__(self, docs_file=None, index_path='index', models_path='models/weights', encoder_batch_size=32):\n",
    "        # Initialization code as in the script provided\n",
    "        self.index_path = index_path\n",
    "        self.encoder_batch_size = encoder_batch_size\n",
    "\n",
    "        device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "\n",
    "        # initialize the sentence tokenizer\n",
    "        self.sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.sent_tokenizer._params.abbrev_types.update(['e.g', 'i.e', 'subsp'])\n",
    "\n",
    "        # initialize the passage embedding model\n",
    "        self.text_embedding_model = SentenceTransformer('all-mpnet-base-v2',\n",
    "                                                        device=device)\n",
    "\n",
    "        if docs_file is None:\n",
    "            if os.path.exists('{}/vectors.pkl'.format(self.index_path)):\n",
    "                self.dense_index = DenseRetriever(model=self.text_embedding_model, batch_size=32)\n",
    "                self.dense_index.create_index_from_vectors('{}/vectors.pkl'.format(index_path))\n",
    "                self.sparse_index = SparseRetrieverFast(path=self.index_path)\n",
    "                self.documents = pickle.load(open('{}/documents.pkl'.format(index_path), 'rb'))\n",
    "\n",
    "        else:\n",
    "            self.index_documents(docs_file=docs_file)\n",
    "\n",
    "    def index_documents(self, docs_file, sentences_per_snippet=5):\n",
    "        # Code for indexing documents\n",
    "        logging.info('Indexing snippets...')\n",
    "\n",
    "        self.documents = {}\n",
    "        all_snippets = []\n",
    "        with open(docs_file, encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                document = json.loads(line.rstrip('\\n'))\n",
    "                snippets = self.extract_snippets(document[\"text\"], sentences_per_snippet)\n",
    "                for snippet in snippets:\n",
    "                    all_snippets.append(snippet)\n",
    "                    self.documents[len(self.documents)] = {\n",
    "                        'snippet': snippet\n",
    "                    }\n",
    "                if i % 1000 == 0:\n",
    "                    logging.info('processed: {} - snippets: {}'.format(i, len(all_snippets)))\n",
    "\n",
    "        # Check if the index_path directory exists, create it if not\n",
    "        if not os.path.exists(self.index_path):\n",
    "            os.makedirs(self.index_path)\n",
    "\n",
    "        pickle.dump(self.documents, open('{}/documents.pkl'.format(self.index_path), 'wb'))\n",
    "\n",
    "        logging.info('Building sparse index...')\n",
    "\n",
    "        self.sparse_index = SparseRetrieverFast(path=self.index_path) \n",
    "        self.sparse_index.index_documents(all_snippets)\n",
    "\n",
    "        logging.info('Building dense index...')\n",
    "\n",
    "        self.dense_index = DenseRetriever(model=self.text_embedding_model,\n",
    "                                          batch_size=self.encoder_batch_size)\n",
    "        self.dense_index.create_index_from_documents(all_snippets)\n",
    "        self.dense_index.save_index(vectors_path='{}/vectors.pkl'.format(self.index_path))\n",
    "\n",
    "        logging.info('Done')\n",
    "\n",
    "    def extract_snippets(self, text, sentences_per_snippet=5):\n",
    "        # Code for extracting snippets from text\n",
    "        sentences = self.sent_tokenizer.tokenize(text)\n",
    "        snippets = []\n",
    "        i = 0\n",
    "        last_index = 0\n",
    "        while i < len(sentences):\n",
    "            snippet = ' '.join(sentences[i:i + sentences_per_snippet])\n",
    "            if len(snippet.split(' ')) > 4:\n",
    "                snippets.append(snippet)\n",
    "            last_index = i + sentences_per_snippet\n",
    "            i += int(math.ceil(sentences_per_snippet / 2))\n",
    "        if last_index < len(sentences):\n",
    "            snippet = ' '.join(sentences[last_index:])\n",
    "            if len(snippet.split(' ')) > 4:\n",
    "                snippets.append(snippet)\n",
    "        return snippets\n",
    "\n",
    "    def search(self, query, limit=100):\n",
    "        # Code for performing a search\n",
    "        logging.info('Running sparse retriever for: {}'.format(query))\n",
    "\n",
    "        sparse_results = self.sparse_index.search([query], topk=limit)[0]\n",
    "        sparse_results = [r[0] for r in sparse_results]\n",
    "\n",
    "        logging.info('Running dense retriever for: {}'.format(query))\n",
    "\n",
    "        dense_results = self.dense_index.search([query], limit=limit)[0]\n",
    "        dense_results = [r[0] for r in dense_results]\n",
    "\n",
    "        results = list(set(sparse_results + dense_results))\n",
    "\n",
    "        # print(sparse_results)\n",
    "        # print(len(self.documents))\n",
    "        search_results = []\n",
    "        if len(results) > 0:\n",
    "#             for i in range(len(results)):\n",
    "#                 doc_id = results[i]\n",
    "#                 result = copy.copy(self.documents[doc_id])\n",
    "#                 search_results.append(result)\n",
    "            \n",
    "            for i in range(len(results)):\n",
    "                doc_id = results[i]\n",
    "                try:\n",
    "                    result = copy.copy(self.documents[doc_id])\n",
    "                    search_results.append(result)\n",
    "                except KeyError as e:\n",
    "                    logging.warning(f\"Document ID {doc_id} not found in `self.documents`.\")\n",
    "                    \n",
    "        paragraphs = search_results.copy()\n",
    "        logging.info('highlighting...')\n",
    "        results_sentences = []\n",
    "        sentences_texts = []\n",
    "        sentences_vectors = {}\n",
    "        for i, r in enumerate(search_results):\n",
    "            sentences = self.sent_tokenizer.tokenize(r['snippet'])\n",
    "            sentences = [s for s in sentences if len(s.split(' ')) > 4]\n",
    "            sentences_texts.extend(sentences)\n",
    "            results_sentences.append(sentences)\n",
    "\n",
    "        vectors = self.text_embedding_model.encode(sentences=sentences_texts, batch_size=128)\n",
    "        for i, v in enumerate(vectors):\n",
    "            sentences_vectors[sentences_texts[i]] = v\n",
    "\n",
    "        query_vector = self.text_embedding_model.encode(sentences=[query], batch_size=1)[0]\n",
    "        for i, sentences in enumerate(results_sentences):\n",
    "            best_sentences = set()\n",
    "            evidence_sentences = []\n",
    "            for sentence in sentences:\n",
    "                sentence_vector = sentences_vectors[sentence]\n",
    "                score = 1 - spatial.distance.cosine(query_vector, sentence_vector)\n",
    "                if score > 0.9:\n",
    "                    best_sentences.add(sentence)\n",
    "                    evidence_sentences.append(sentence)\n",
    "            if len(evidence_sentences) > 0:\n",
    "                search_results[i]['evidence'] = ' '.join(evidence_sentences)\n",
    "            search_results[i]['snippet'] = \\\n",
    "                ' '.join([s if s not in best_sentences else '<b>{}</b>'.format(s) for s in sentences])\n",
    "\n",
    "        search_results = [s for s in search_results if 'evidence' in s]\n",
    "\n",
    "        search_results = search_results[:limit]\n",
    "        paragraphs = paragraphs[:limit]\n",
    "        logging.info('done searching')\n",
    "        return search_results,paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaedec3",
   "metadata": {},
   "source": [
    "## Initiate Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b350ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "INFO:root:Indexing snippets...\n",
      "INFO:root:processed: 0 - snippets: 1\n",
      "INFO:root:processed: 1000 - snippets: 8375\n",
      "INFO:root:processed: 2000 - snippets: 15853\n",
      "INFO:root:processed: 3000 - snippets: 23321\n",
      "INFO:root:processed: 4000 - snippets: 31179\n",
      "INFO:root:processed: 5000 - snippets: 38908\n",
      "INFO:root:processed: 6000 - snippets: 46622\n",
      "INFO:root:processed: 7000 - snippets: 53578\n",
      "INFO:root:processed: 8000 - snippets: 61330\n",
      "INFO:root:processed: 9000 - snippets: 68736\n",
      "INFO:root:processed: 10000 - snippets: 77169\n",
      "INFO:root:processed: 11000 - snippets: 84716\n",
      "INFO:root:processed: 12000 - snippets: 92289\n",
      "INFO:root:processed: 13000 - snippets: 100265\n",
      "INFO:root:processed: 14000 - snippets: 107924\n",
      "INFO:root:processed: 15000 - snippets: 115464\n",
      "INFO:root:processed: 16000 - snippets: 123017\n",
      "INFO:root:processed: 17000 - snippets: 131812\n",
      "INFO:root:processed: 18000 - snippets: 138868\n",
      "INFO:root:processed: 19000 - snippets: 146836\n",
      "INFO:root:Building sparse index...\n",
      "INFO:root:Building sparse index of 154446 docs...\n",
      "INFO:root:Indexed 100000 docs\n",
      "INFO:root:Built sparse index\n",
      "INFO:root:Building dense index...\n",
      "INFO:root:Building index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11ca849bf5f45bf8d6f2ee5f8528c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4827 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Indexing 154446 vectors\n",
      "INFO:root:Using 2896 centroids\n",
      "INFO:root:Training index...\n",
      "INFO:root:Adding vectors to index...\n",
      "INFO:root:Built index\n",
      "INFO:root:Done\n"
     ]
    }
   ],
   "source": [
    "docs_file = 'data/polusa2019.jsonl'  # Example path to documents file\n",
    "index_path = 'index'\n",
    "models_path = 'models/weights'\n",
    "encoder_batch_size = 32\n",
    "limit = 10\n",
    "\n",
    "# Instantiate the NewsRetriever\n",
    "q = NewsRetriever(docs_file=docs_file,\n",
    "                  index_path=index_path,\n",
    "                  models_path=models_path,\n",
    "                  encoder_batch_size=encoder_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519b539",
   "metadata": {},
   "source": [
    "## Summarized Evidence (Limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69ba63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to summarize evidence for a single claim\n",
    "def summarize_evidence_for_claim(claim, q, summarizer):\n",
    "    # Perform search to retrieve top 5 pieces of evidence\n",
    "    _, search_results = q.search(claim, limit=5)\n",
    "\n",
    "    # Extract the evidence snippets\n",
    "    evidence_texts = [result['snippet'] for result in search_results]\n",
    "\n",
    "    # Join the evidence snippets into one text block\n",
    "    evidence_text_block = \"\\n\".join(evidence_texts)\n",
    "\n",
    "    # Generate the summary for the evidence\n",
    "    summary = summarizer(evidence_text_block, max_length=200, min_length=100, do_sample=False)\n",
    "\n",
    "    # Return the summarized evidence\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# Initialize a summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Read all claims from the file\n",
    "with open('data/liar_claims.txt', 'r') as file:\n",
    "    claims = file.readlines()\n",
    "\n",
    "# Open the output file for writing\n",
    "with open('liar_claims_evidence_summarized.txt', 'w') as outfile:\n",
    "    for claim in claims:\n",
    "        claim = claim.strip()\n",
    "        if not claim:  # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Summarize evidence for the current claim\n",
    "            summarized_evidence = summarize_evidence_for_claim(claim, q, summarizer)\n",
    "            \n",
    "            # Write the claim and its summarized evidence to the output file\n",
    "            outfile.write(f\"Claim: {claim}\\n\")\n",
    "            outfile.write(f\"Evidence Summary: {summarized_evidence}\\n\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing claim: {claim}\\n{e}\\n\")\n",
    "\n",
    "print(\"Processing complete. Summarized evidence written to liar_claims_evidence.txt.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
